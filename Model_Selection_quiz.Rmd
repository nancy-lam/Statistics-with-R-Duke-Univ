---
title: "Capstone Quiz II"
output: statsr:::statswithr_lab
---

<div id="instructions">
Complete all **Exercises**, and submit answers to **Questions** on the Coursera 
platform.
</div>

This second quiz will deal with model assumptions, selection, and interpretation.  The concepts tested here will prove useful for the final peer assessment, which is much more open-ended.

First, let us load the data:

```{r load, warning= FALSE, message=FALSE}
load("ames_train.Rdata")
library(dplyr)
library(ggplot2)
library(MASS)

```


1. Suppose you are regressing $\log$(`price`) on $\log$(`area`), $\log$(`Lot.Area`), `Bedroom.AbvGr`, `Overall.Qual`, and `Land.Slope`.  Which of the following variables are included with stepwise variable selection using AIC but not BIC?  Select all that apply.
<ol>
<li> $\log$(`area`) </li>
<li> $\log$(`Lot.Area`) </li> 
<li> `Bedroom.AbvGr` </li> 
<li> `Overall.Qual` </li>
<li> **`Land.Slope`** </li>
</ol>


```{r Q1}
data <- ames_train %>% dplyr:: select(price, area, Lot.Area, Bedroom.AbvGr, Overall.Qual, Land.Slope)

data.model <- data[complete.cases(data),]

full <- lm(log(price) ~ log(area) + log(Lot.Area) + Bedroom.AbvGr + Overall.Qual + Land.Slope, data = data.model)
step <- stepAIC(full, trace = FALSE, k = 2)
step$anova

```

```{r}
BIC <- stepAIC(full, trace = FALSE, k = log(nrow(data.model)))
BIC$anova
```


\fb{Use the function `stepAIC` from the `MASS` package.  In AIC, $k = 2$, whereas in BIC, $k = \log(n)$.

This question refers to the following learning objective(s):
Use principled statistical methods to select a single parsimonious model.
}


2. When regressing $\log$(`price`) on `Bedroom.AbvGr`, the coefficient for `Bedroom.AbvGr` is strongly positive.  However, once $\log$(`area`) is added to the model, the coefficient for `Bedroom.AbvGr` becomes strongly negative.  Which of the following best explains this phenomenon?
<ol>
<li> The original model was misspecified, biasing our coefficient estimate for `Bedroom.AbvGr`
<li> Bedrooms take up proportionally less space in larger houses, which increases property valuation.
<li> **Larger houses on average have more bedrooms and sell for higher prices.  However, holding constant the size of a house, the number of bedrooms decreases property valuation.**
<li> Since the number of bedrooms is a statistically insignificant predictor of housing price, it is unsurprising that the coefficient changes depending on which variables are included.
</ol>

```{r Q2}
# regressing $\log$(`price`) on `Bedroom.AbvGr`
m1 <- lm(log(price) ~ Bedroom.AbvGr, data = data.model)
summary(m1)

m2 <- lm(log(price) ~ Bedroom.AbvGr + log(area), data = data.model)
summary(m2)

```

\fb{Recall: the interpretation of a coefficient holds constant all other variables included in the model.

This question refers to the following learning objective(s):
Interpret the estimate for a slope (say $$b_1$$) as "All else held constant, for each unit increase in $$x_1$$, we would expect $$y$$ to be higher/lower on average by $$b_1$$."
}

3.  Run a simple linear model for $\log$(`price`), with $\log$(`area`) as the independent variable.  Which of the following neighborhoods has the highest average residuals?  
<ol>
<li> `OldTown`
<li> `StoneBr`
<li> **`GrnHill`**
<li> `IDOTRR`
</ol>

```{r Q3}
model3 <- lm(log(price) ~ log(area), data = data.model)
predict3 <- predict(model3, newdata = ames_train)
ames_train$resid <- resid(model3)
ames_train %>%
  group_by(Neighborhood) %>%
  summarize(mean_resid = mean(resid)) %>%
  arrange(desc(mean_resid))

```

\fb{Extract the residual from a model (say `m.1`) with the command `resid(m.1)`.  Then summarize the residuals, grouping by 
neighborhood.

This question refers to the following learning objective(s):
Identify the assumptions of linear regression and assess when a model may need to be improved.
Examine the residuals of a linear model.
}

4. We are interested in determining how well the model fits the data for each neighborhood.  The model from Question 3 does the worst at predicting prices in which of the following neighborhoods?
<ol>
<li> **`GrnHill`**
<li> `BlueSte`
<li> `StoneBr`
<li> `MeadowV`
</ol>

```{r Q4}
model4 <- lm(log(price) ~ log(area), data = data.model)
predict4 <- predict(model4, newdata = ames_train)
ames_train$resid <- resid(model4)
ames_train %>%
  group_by(Neighborhood) %>%
  summarize(mean_r.squared = mean(resid^2)) %>%
  arrange(desc(mean_r.squared))


```

\fb{The average squared residuals is one good measure for comparing how well the model predicts prices between neighborhoods.  Find the neighborhood for which this is maximized.

This question refers to the following learning objective(s):
Examine the residuals of a linear model.
}


5. Suppose you want to model  $\log$(`price`) using only the variables in the dataset that pertain to quality: `Overall.Qual`, `Basement.Qual`, and `Garage.Qual`.  How many observations must be discarded in order to estimate this model?
<ol>
<li> 0
<li> 46
<li> **64**
<li> 924
</ol>

```{r Q5}
data5 <- ames_train %>% dplyr:: select(Overall.Qual, Bsmt.Qual, Garage.Qual, price)

# To perform linear regression model, we will exclude NA
data5.model <- data5[complete.cases(data5),]

print(paste('Number of observations must be discarded in order to estimate this model is', nrow(data5) - nrow(data5.model)))

```

\fb{Run the model in R.  How many observations are used?  There are 1000 originally in the data.

This question refers to the following learning objective(s):
Identify the assumptions of linear regression and assess when a model may need to be improved.
}

6.  `NA` values for `Basement.Qual` and `Garage.Qual` correspond to houses that do not have a basement or a garage respectively.  Which of the following is the best way to deal with these `NA` values when fitting the linear model with these variables?    
<ol>
<li> Drop all observations with `NA` values for `Basement.Qual` or `Garage.Qual` since the model cannot be estimated otherwise.
<li> Recode all `NA` values as the category `TA` since we must assume these basements or garages are typical in the absence of all other information.
<li> **Recode all `NA` values as a separate category, since houses without basements or garages are fundamentally different than houses with both basements and garages**
</ol>

```{r Q6}
data6 <- ames_train %>% dplyr:: select(Bsmt.Qual, Garage.Qual, price)
data6.model <- data6[complete.cases(data6),]
data6_na <- is.na(data6)

```

\fb{We can use the information that a house does not have a basement or garage to create a separate category for `NA` values.  This will allow us to avoid discarding observations and use the model to predict prices for out-of-sample houses that lack garages or basements.

This question refers to the following learning objective(s):
Check the assumptions of a linear model.
}

7. Run a simple linear model with  $\log$(`price`) regressed on `Overall.Cond` and `Overall.Qual`.  Which of the following subclasses of dwellings (`MS.SubClass`) has the highest median predicted prices?
<ol>
<li> **075: 2-1/2 story houses**
<li> 060: 2 story, 1946 and Newer
<li> 120: 1 story planned unit development
<li> 090: Duplexes
</ol>

```{r Q7}
lm7 = lm(log(price) ~ Overall.Cond + Overall.Qual, data = ames_train)
pred7 = predict(lm7, newdata = ames_train)
ames_train$pred7 = exp(pred7)
ames_train %>% group_by(MS.SubClass) %>% summarise(median_price=median(pred7)) %>% arrange(desc(median_price))
```

\fb{After fitting the model, use the `predict` function to find the predicted values for each observation in the data set.  Then use `group_by` and `summarise` in dplyr to find the median predicted price for each subclass of dwellings.

This question refers to the following learning objective(s):
Predict the value of the response variable for a given value of the explanatory variable, $x^\star$, by plugging in $x^\star$ in the linear model:
}

8. Using the model from Question 7, which observation has the highest leverage or influence on the regression model?  Hint: use `hatvalues`, `hat` or `lm.influence`.
<ol>
<li> 125
<li> **268**
<li> 640
<li> 832
</ol>

```{r Q8}
which.max(lm.influence(lm7)$hat)
```

\fb{First, fit the model from question 7.  Then, using `hatvalues` or a combination of `hat` and `lm.influence`, you can extract the leverage values of the model.  The higher the leverage, the more influential the observation is on the model fit.

This question refers to the following learning objective(s):
Identify outliers and high leverage points in a linear model.
}

9. Which of the following corresponds to a correct interpretation of the coefficient $k$ of `Bedroom.AbvGr`, where  $\log$(`price`) is the dependent variable?
<ol>
<li> Holding constant all other variables in the dataset, on average, an additional bedroom will increase housing price by $k$ percent.
<li> **Holding constant all other variables in the model, on average, an additional bedroom will increase housing price by $k$ percent.**
<li> Holding constant all other variables in the dataset, on average, an additional bedroom will increase housing price by $k$ dollars.
<li> Holding constant all other variables in the model, on average, an additional bedroom will increase housing price by $k$ dollars.
</ol>

```{r Q9}
model9 <- lm(log(price) ~ Bedroom.AbvGr, ames_train)
coef(model9)

```

\fb{In a multiple regression setting, we only hold constant all other variables included in the model.  Also, since we use `log(price)` as our dependent variable, we interpret the coefficient as a percent increase or decrease, rather than an absolute increase or decrease.

This question refers to the following learning objective(s):
Interpret the estimate for a slope (say $$b_1$$) as "All else held constant, for each unit increase in $$x_1$$, we would expect $$y$$ to be higher/lower on average by $$b_1$$."
}

In a linear model, we assume that all observations in the data are generated from the same process.  You are concerned that houses sold in abnormal sale conditions may not exhibit the same behavior as houses sold in normal sale conditions.  To visualize this, you make the following plot of 1st and 2nd floor square footage versus log(price):

```{r conditionPlot}
n.Sale.Condition = length(levels(ames_train$Sale.Condition))
par(mar=c(5,4,4,10))
plot(log(price) ~ I(X1st.Flr.SF+X2nd.Flr.SF), 
     data=ames_train, col=Sale.Condition,
     pch=as.numeric(Sale.Condition)+15, main="Training Data")
legend(x=,"right", legend=levels(ames_train$Sale.Condition),
       col=1:n.Sale.Condition, pch=15+(1:n.Sale.Condition),
       bty="n", xpd=TRUE, inset=c(-.5,0))
```

10. Which of the following sale condition categories shows significant differences from the normal selling condition?
<ol>
<li> `Family`
<li> `Abnorm`
<li> `Partial`
<li> **`Abnorm` and `Partial`** (*p-value for intercept and partial is the same which is <2e-16*)
</ol>

```{r Q10}
model10 <- lm(log(price) ~ Sale.Condition, data = ames_train)
summary(model10)

```


\fb{A house sold under abnormal conditions often sells for much less than expected given its square footage.  Similarly, a partial sale of a house results in a higher price on average holding constant square footage.  However, one partial sale has a total square footage of over 4000, which is highly influential on the regression results.

This question refers to the following learning objective(s):
Be cautious about using a categorical explanatory variable when one of the levels has very few observations, as these may act as influential points.
List the conditions for multiple linear regression.
}


Because houses with non-normal selling conditions exhibit a typical behavior and can disproportionately influence the model, you decide to only model housing prices under only **normal sale conditions**.  

11. Subset `ames_train` to only include houses sold under normal sale conditions.  What percent of the original observations remain?
<ol>
<li> 81.2\%
<li> 83.4\%
<li> 87.7\%
<li> 91.8\%
</ol>

```{r Q11}
normal <- subset(ames_train, Sale.Condition == 'Normal')
print(paste('% of the original observations remain =', nrow(normal)/1000*100))

```

\fb{Use either `dplyr` or the `subset` R command, setting the logical condition to be `Sale.Condition == T`.

This question refers to the following learning objective(s):
Use R commands to effectively manipulate data.
}

12. Now re-run the simple model from question 3 on the subsetted data.  True or False: Modeling only the normal sales results in a better model fit than modeling all sales (in terms of $R^2$).
<ol>
<li> **True, restricting the model to only include observations with normal sale conditions increases the $R^2$ from 0.547 to 0.575.**
<li> True, restricting the model to only include observations with normal sale conditions increases the $R^2$ from 0.575 to 0.603.
<li> False, restricting the model to only include observations with normal sale conditions decreases the $R^2$ from 0.575 to 0.547.
<li> False, restricting the model to only include observations with normal sale conditions decreases the $R^2$ from 0.603 to 0.575.
</ol>

```{r Q12}
model12 <- lm(log(price) ~ log(area), data = normal)
summary(model12)$adj.r.squared
model <- lm(log(price) ~ log(area), data = ames_train)
summary(model)$adj.r.squared
```


\fb{Run the model under both the full and subsetted data.  Calculate the $R^2$ values for each model and compare.

This question refers to the following learning objective(s):
Be cautious about using a categorical explanatory variable when one of the levels has very few observations, as these may act as influential points.
Define $R^2$ as the percentage of the variability in the response variable explained by the the explanatory variable.

}
